{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a8fcdc7",
   "metadata": {},
   "source": [
    "# Model Context Protocol (MCP) Integratiion\n",
    "\n",
    "This notebook demonstrates how to combine MCP Servers with the Langchain + Snowflake integration to leverage Snowflake Cortex models as the orchestrating model in MCP.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **MCP Tools** - Unpacking MCP server tools as Langchain tools\n",
    "2. **MCP Server Connection** - Connect Langchain chat with an MCP servers \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed `getting_started.ipynb` and `snowflake_workflows.ipynb`\n",
    "- Understanding of LangChain basics (chat models, tools, chains)\n",
    "- Understanding of Model Context Protocol\n",
    "\n",
    "## Why These Patterns Matter\n",
    "\n",
    "The Langchain - Snowflake integration is the first open-source framework that combines Model Context Protocol with Snowflake Cortex LLMs as the orchestrating models. This combination allows an end-to-end MCP ecosystem to exist entirely within the Snowflake security perimeter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48450d99",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Install additional package:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbfb3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --quiet -U langchain-mcp-adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ac810",
   "metadata": {},
   "source": [
    "[langchain-mcp-adapters](https://github.com/langchain-ai/langchain-mcp-adapters) provides a lightweight wrapper that makes MCP Server tools compatible with LangChain binded tools. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2272c3a9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ba194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup\n",
    "from langchain_snowflake import ChatSnowflake, create_session_from_env\n",
    "from langchain_core.tools import tool\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from mcp import ClientSession\n",
    "\n",
    "\n",
    "# Initialize session and LLM - consistent with other notebooks\n",
    "session = create_session_from_env()\n",
    "llm = ChatSnowflake(\n",
    "    session=session, model=\"claude-4-sonnet\", temperature=0.1, max_tokens=2000\n",
    ")\n",
    "\n",
    "print(\"Session and LLM initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ba9f1d",
   "metadata": {},
   "source": [
    "## MCP Server\n",
    "We will use the [open-source MCP Server for Snowflake](https://github.com/Snowflake-Labs/mcp/tree/main) as an example. The MCP Server supports `stdio`, `streamable-http`, and `sse` transports. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cada4b",
   "metadata": {},
   "source": [
    "## 1. Synchronous Tool Calling with STDIO MCP Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c37aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"uvx\",\n",
    "    args=[\n",
    "        \"--from\",  \n",
    "        \"git+https://github.com/Snowflake-Labs/mcp\", \n",
    "        \"mcp-server-snowflake\", \n",
    "        \"--service-config-file\", \n",
    "        \"<path-to-mcp-config-file>.yaml\", \n",
    "        \"--connection-name\", \n",
    "        \"default\", \n",
    "        \"--transport\",\n",
    "        \"stdio\"],\n",
    ")\n",
    "\n",
    "prompt = \"create a transient database named my_langchain_db with replacement\"\n",
    "\n",
    "async with stdio_client(server_params) as (read, write):\n",
    "    async with ClientSession(read, write) as client_session:\n",
    "        \n",
    "        # Initialize the connection\n",
    "        await client_session.initialize()\n",
    "\n",
    "        # Get tools\n",
    "        tools = await load_mcp_tools(client_session)\n",
    "        agent = llm.bind_tools(tools, auto_execute=False)\n",
    "\n",
    "        # Use sync invoke for the LLM, but handle async tools manually\n",
    "        response = agent.invoke(prompt)\n",
    "        print(\"Initial response:\", response)\n",
    "        \n",
    "        # If tools were called, execute them manually\n",
    "        if hasattr(response, 'tool_calls') and response.tool_calls:\n",
    "            print(f\"Tools called: {len(response.tool_calls)}\")\n",
    "            for tool_call in response.tool_calls:\n",
    "                tool_name = tool_call['name']\n",
    "                tool_args = tool_call['args']\n",
    "                print(f\"Executing tool: {tool_name} with args: {tool_args}\")\n",
    "                \n",
    "                # Find and execute the tool\n",
    "                for tool in tools:\n",
    "                    if tool.name == tool_name:\n",
    "                        try:\n",
    "                            result = await tool.ainvoke(tool_args)\n",
    "                            print(f\"Tool {tool_name} result: {result}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error executing {tool_name}: {e}\")\n",
    "                        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4a6567",
   "metadata": {},
   "source": [
    "## 2. Synchronous Tool Calling with HTTP-streaming MCP Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390a24d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "\n",
    "\n",
    "prompt = \"create a transient database named my_langchain_db with replacement\"\n",
    "\n",
    "async with streamablehttp_client(\"http://0.0.0.0:9000/mcp/\") as (read, write, _):\n",
    "    async with ClientSession(read, write) as client_session:\n",
    "        # Initialize the connection\n",
    "        await client_session.initialize()\n",
    "\n",
    "        # Get tools\n",
    "        tools = await load_mcp_tools(client_session)\n",
    "        agent = llm.bind_tools(tools, auto_execute=False)\n",
    "\n",
    "        # Use sync invoke for the LLM, but handle async tools manually\n",
    "        response = agent.invoke(prompt)\n",
    "        print(\"Initial response:\", response)\n",
    "        \n",
    "        # If tools were called, execute them manually\n",
    "        if hasattr(response, 'tool_calls') and response.tool_calls:\n",
    "            print(f\"Tools called: {len(response.tool_calls)}\")\n",
    "            for tool_call in response.tool_calls:\n",
    "                tool_name = tool_call['name']\n",
    "                tool_args = tool_call['args']\n",
    "                print(f\"Executing tool: {tool_name} with args: {tool_args}\")\n",
    "                \n",
    "                # Find and execute the tool\n",
    "                for tool in tools:\n",
    "                    if tool.name == tool_name:\n",
    "                        try:\n",
    "                            result = await tool.ainvoke(tool_args)\n",
    "                            print(f\"Tool {tool_name} result: {result}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error executing {tool_name}: {e}\")\n",
    "                        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829425fa",
   "metadata": {},
   "source": [
    "## 3. Asynchronous Tool Calling with HTTP-streaming MCP Server\n",
    "### Coming Soon"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
